{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKtoITdyusB5"
   },
   "outputs": [],
   "source": [
    "# @title Pull Local RAG repository https://github.com/relyativist/rag-gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMc_lnUjuaUD",
    "outputId": "b1f38b19-1076-415f-842e-1022e4cbe482"
   },
   "outputs": [],
   "source": [
    "!echo $PWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnbxhLQYuIJz",
    "outputId": "0c1d8ed2-d138-403d-de9b-dec5c553724d"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "  echo \"Installing apt packages ...\\n\" && \\\n",
    "  apt-get install -y tree && \\\n",
    "  echo \"Clonning rag git repos ... \\n\" && \\\n",
    "  git clone https://github.com/relyativist/rag-gain.git code/rag-gain && \\\n",
    "  echo $PWD/code\n",
    "  #rm -r /content/sample_data\n",
    "  #tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2a-fXn6wvnG"
   },
   "source": [
    "## Implement .pdf parser, chunking and embed chunks\n",
    "\n",
    "Parse .pdf doc of choice. ([Feature]: Any kind of document)\n",
    "\n",
    "Steps to implement:\n",
    "\n",
    "\n",
    "\n",
    "1.   Import .pdf;\n",
    "2.   Parse text;\n",
    "3.   Implement multiple chunking strategies to compare performance:\n",
    "  3.1 Paragraph chunking;\n",
    "  3.2 Fixed size chunking;\n",
    "  3.2 LangChain (e.g. [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter/), [SemanticChanking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker/))\n",
    "4.   Embedding with SentenceTransformers\n",
    "5.   Populate lance .db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znllkdyZK8GM"
   },
   "source": [
    "Example *.pdf*s are taken from europarl.europa.eu on AI regulations;\n",
    "\n",
    "[AIDA public hearing on AI and the Data Strategy of 30 September 2021 (PDF - 458 KB)](https://www.europarl.europa.eu/cmsdata/242360/AIDA_Verbatim_30_September_2021_EN.pdf)\n",
    "\n",
    "[AIDA-AGRI public hearing on AI in agriculture and food security of 14 June 2021 (PDF - 401 KB)](https://www.europarl.europa.eu/cmsdata/238861/AIDA_Verbatim_14_June_2021_EN.pdf)\n",
    "\n",
    "[AIDA-EMPL public hearing on AI and the Labour Market of 25 May 2021 (PDF - 453 KB)](https://www.europarl.europa.eu/cmsdata/238560/AIDA_Verbatim_25_May_2021_EN.pdf)\n",
    "\n",
    "[AIDA public hearing on AI and Health of 2 December 2020 (PDF - 550 KB)](https://www.europarl.europa.eu/cmsdata/222231/AIDA_Verbatim_2_December_2020_EN.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DGu7M-Zu_0e"
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from spacy.lang.en import English\n",
    "import pdb\n",
    "\n",
    "\n",
    "def import_pdf(path):\n",
    "    \"\"\"\n",
    "    Imports pdf from path, if not exists download files predefined urls\n",
    "    args:\n",
    "      path : str - absolute path to pdf document\n",
    "\n",
    "    return : str - pdf paths\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_path = Path(path)\n",
    "\n",
    "    if not pdf_path.is_file():\n",
    "        urls = [\"https://www.europarl.europa.eu/cmsdata/242360/AIDA_Verbatim_30_September_2021_EN.pdf\",\n",
    "              \"https://www.europarl.europa.eu/cmsdata/238861/AIDA_Verbatim_14_June_2021_EN.pdf\",\n",
    "              \"https://www.europarl.europa.eu/cmsdata/238560/AIDA_Verbatim_25_May_2021_EN.pdf\",\n",
    "              \"https://www.europarl.europa.eu/cmsdata/222231/AIDA_Verbatim_2_December_2020_EN.pdf\"]\n",
    "\n",
    "        for url in urls:\n",
    "            filename = url.split(\"/\")[-1]\n",
    "            response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            with open(filename, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "        else:\n",
    "            return response.status_code\n",
    "    else:\n",
    "        return pdf_path\n",
    "\n",
    "\n",
    "def text_processor(text: str) -> str:\n",
    "\n",
    "    proc_text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    return proc_text\n",
    "\n",
    "\n",
    "def pdf_parser(pdf_path : str) -> List[Dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "\n",
    "    for _, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_processor(text)\n",
    "        pages.append({\"page_chars\": len(text),\n",
    "                    \"page_words\": len(text.split(\" \")),\n",
    "                    \"page_sentences_naive\": len(text.split(\". \")),\n",
    "                    \"page_tokens\": len(text) / 4,  # ~ 1 token is 4 chars https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                    \"text\": text\n",
    "                    })\n",
    "    \n",
    "    return pages\n",
    "\n",
    "\n",
    "def custom_textsplitter(text, chunk_size : int = 4):\n",
    "\n",
    "    \"\"\"\n",
    "    Group sentences in chunks\n",
    "    args:\n",
    "      text : str\n",
    "      chunk_size : int  - number of sentences in one chunk, e.g. [25] -> [int, int ,mod]\n",
    "    return : List\n",
    "    \"\"\"\n",
    "\n",
    "    nlp = English()\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "    dot_sep_list = list(nlp(text).sents)\n",
    "    dot_sep_str = [str(x) for x in dot_sep_list]\n",
    "    \n",
    "    sentence_chunks = [dot_sep_str[i:i + chunk_size] for i in range(0, len(dot_sep_str), chunk_size)]\n",
    "\n",
    "    text_chunks = []\n",
    "\n",
    "    for chunk in sentence_chunks:\n",
    "        chunkd = {}\n",
    "        joined_chunk = \"\".join(chunk).replace(\"  \", \" \").strip()\n",
    "        joined_chunk = re.sub(r\"\\.([A-Z])\", r\". \\1\", joined_chunk)  # add space to sentence ending dot, e.g. .A => . A\n",
    "        chunkd[\"sentence_chunk\"] = joined_chunk\n",
    "        chunkd[\"chunk_char_count\"] = len(joined_chunk)\n",
    "        chunkd[\"chunk_word_count\"] = len([word for word in joined_chunk.split(\" \")])\n",
    "        chunkd[\"chunk_sentence_count\"] = len(joined_chunk.split(\".\"))\n",
    "        chunkd[\"chunk_token_count\"] = len(joined_chunk) / 4\n",
    "        text_chunks.append(chunkd)\n",
    "\n",
    "    return text_chunks\n",
    "\n",
    "\n",
    "def lc_rec_textsplitter(text, chunk_size):\n",
    "    \"\"\"\n",
    "     The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs\n",
    "    (and then sentences, and then words) together as long as possible\n",
    "    \"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,  # mean doc token len is 836, with maximum of 7549, and min of 130\n",
    "        chunk_overlap = 100,\n",
    "        length_function = len,\n",
    "        add_start_index = True,  # just to check the index of particular chunk\n",
    "    )\n",
    "    lc_textsplitter = text_splitter.split_text(text)\n",
    "    return lc_textsplitter\n",
    "\n",
    "\n",
    "def chunking(\n",
    "        text : str,\n",
    "        fixed_chunk_size : int = 256,\n",
    "        sentence_chunk_size : int = 4,\n",
    "        method : str =\"langchain\"\n",
    "        ) -> List[str]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Chunk inpurt text, with method defined.\n",
    "    In case of \"custom\" method, the sentence_chunk_size is used as the number of sentences in one chunk, e.g. 10 sentences in one chunk\n",
    "    In case of \"langchain\" method, the fixed_chunk_size defines the number of chars in one chunk, e.f. 256 in one chunk\n",
    "    args:\n",
    "      text : str\n",
    "      fixed_chunk_size : int\n",
    "      sentence_chunk_size : int\n",
    "      method : str\n",
    "    return : str List of lists of chunks\n",
    "    \"\"\"\n",
    "\n",
    "    if method == \"langchain\":\n",
    "        chunks = lc_rec_textsplitter(text, fixed_chunk_size)\n",
    "    if method == \"custom\":\n",
    "        chunks = custom_textsplitter(text, sentence_chunk_size : = 10)\n",
    "    else:\n",
    "        raise AttributeError(\n",
    "            \"\"\"\n",
    "            wrong chunking method: choose from supported options: \\n\n",
    "            - 'fixed_sized' \\n\n",
    "            - 'paragraph' \\n\n",
    "            - 'langchain' \\n\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "doc_regex = r\"\\bAIDA.*\\.pdf\\b\"\n",
    "\n",
    "directory = \"/code/rag-gain\"\n",
    "\n",
    "doc_lists = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "  \n",
    "    if re.match(doc_regex, file):\n",
    "        pdf_sample = import_pdf(f\"{directory}/{file}\")\n",
    "        pdf_page_texts = pdf_parser(pdf_sample)\n",
    "        for item in tqdm(pdf_page_texts):\n",
    "            item[\"sentence_chunks\"] = chunking(item[\"text\"], method=\"custom\")\n",
    "            item[\"page_sentences_splitter\"] = len(item[\"sentence_chunks\"])\n",
    "        doc_lists.append(pdf_page_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedder \n",
    "\n",
    "Embed text from *.pdfs and with SentenceTransformers and populate LanceDB\n",
    "Input: doc_lists\n",
    "Output: .lancedb vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lancedb\n",
    "import pyarrow as pa\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# DEFINE variables for sentence embedder and lanceDB\n",
    "\n",
    "SENTS_EMBEDDER_MODEL = \"all-mpnet-base-v2\"                   #  sentence embedder model  \n",
    "\n",
    "sents_embedder = SentenceTransformer(SENTS_EMBEDDER_MODEL, device=\"cuda\")\n",
    "sents_embedder.eval()\n",
    "                   \n",
    "BATCH_SIZE = 32                                               #  batches for embedder, we don't want to embed all sentences at once\n",
    "LANCE_DB_LOC = \"./.lancedb\"                      #  location for .lancedb on host\n",
    "NUM_SUB_VEC = sents_embedder.max_seq_length                   #  max token length of embedder model\n",
    "LANCE_DB_TABLENAME = SENTS_EMBEDDER_MODEL + f\"_{NUM_SUB_VEC}\" #  vector table name\n",
    "EMBEDDING_DIM_MODEL = 768                                     #  Transformer embedding dimension\n",
    "NUM_PARTITIONS_VEC = 128\n",
    "NUM_SUB_VEC = 96\n",
    "VEC_COLUMN = \"vector\"                                         #  vector table embeddings column name\n",
    "TEXT_COLUMN = \"text\"                                          #  vector table text column name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all sentences from all documents to sentences -> List[str]: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "all_docs_embeds = []\n",
    "sentences = []\n",
    "for doc in tqdm(doc_lists):\n",
    "    #for item in pdf_page_texts:\n",
    "    for pages in doc:\n",
    "        for chunk in pages[\"sentence_chunks\"]:\n",
    "            chunk[\"chunk_embeds\"] = sents_embedder.encode(chunk[\"sentence_chunk\"], convert_to_tensor=True)\n",
    "            sentences.append(chunk[\"sentence_chunk\"])\n",
    "            all_docs_embeds.append(chunk[\"chunk_embeds\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate DB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert EMBEDDING_DIM_MODEL % NUM_SUB_VEC == 0, \\\n",
    "        \"Embedding size must be divisible by the num of sub vectors\"\n",
    "\n",
    "db = lancedb.connect(LANCE_DB_LOC)\n",
    "\n",
    "schema = pa.schema(\n",
    "      [\n",
    "          pa.field(VEC_COLUMN, pa.list_(pa.float32(), EMBEDDING_DIM_MODEL)),\n",
    "          pa.field(TEXT_COLUMN, pa.string())\n",
    "      ]\n",
    "    )\n",
    "\n",
    "tbl = db.create_table(LANCE_DB_TABLENAME, schema=schema, mode=\"overwrite\")\n",
    "#pdb.set_trace()\n",
    "for i in tqdm(range(0, int(np.ceil(len(sentences) / BATCH_SIZE)))):\n",
    "    try:\n",
    "        batch = [sent for sent in sentences[i * BATCH_SIZE:(i + 1) * BATCH_SIZE] if len(sent) > 0]\n",
    "        encoded = sents_embedder.encode(batch, normalize_embeddings=True, device=\"cuda\")\n",
    "        encoded = [list(vec) for vec in encoded]\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            VEC_COLUMN: encoded,\n",
    "            TEXT_COLUMN: batch\n",
    "        })\n",
    "        tbl.add(df)\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "tbl.create_index(\n",
    "        num_partitions=NUM_PARTITIONS_VEC,\n",
    "        num_sub_vectors=NUM_SUB_VEC,\n",
    "        vector_column_name=VEC_COLUMN\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect(LANCE_DB_LOC)\n",
    "table = db.open_table(LANCE_DB_TABLENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_model = SentenceTransformer(SENTS_EMBEDDER_MODEL, device=\"cpu\")\n",
    "def search(query, top_k = 20):\n",
    "\n",
    "    query_vector = query_model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "    search_results = table.search(query_vector).limit(top_k)\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_examples_for_test =  [\" * Rules on artificial intelligence in healthcare\\n\",\n",
    "                            \" * Artificial intelligence in agriculture\\n\",\n",
    "                            \" * Policies in data privacy\\n\",\n",
    "                            \" * AI and labour market\\n\"]\n",
    "\n",
    "print(\"[EXAMPLES]:\\n\")\n",
    "for query in query_examples_for_test:\n",
    "    print(query)\n",
    "\n",
    "print(\"[QUERY]: Enter query to vector DB ->\\n\")\n",
    "query = str(input())\n",
    "print(f\"[USER QUERY]:\\n{query}\\n\")\n",
    "\n",
    "search_results = search(query, top_k = 20).to_pandas().dropna(subset = \"text\").reset_index(drop=True)\n",
    "print(f\"[Vector DB search]:\\n\")\n",
    "for t in range(len(search_results.text)):\n",
    "    print(search_results.text[t])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(query, top_k = 20).to_pandas().dropna(subset = \"text\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results[\"old_similarity_rank\"] = search_results.sort_values(\"_distance\", ascending=False).index+1\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "\n",
    "reranker_model_name = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "cross_encoder_model = CrossEncoder(reranker_model_name, device=\"cuda\")\n",
    "\n",
    "def rerank(query, search_results, K : int = 5):\n",
    "    query_retrieve_comb = [[query, sent] for sent in search_results[\"text\"]]\n",
    "    search_results[\"_distance_reranked\"] = cross_encoder_model.predict(query_retrieve_comb, activation_fct=torch.nn.Sigmoid())\n",
    "    topk = search_results.sort_values(\"_distance_reranked\", ascending=False).head(K)\n",
    "    return topk\n",
    "\n",
    "new_df = rerank(query, search_results)\n",
    "\n",
    "print(f\"[Reranked Vector DB search]:\\n\")\n",
    "\n",
    "for t in new_df.text:\n",
    "    print(t)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
